{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import requests\n",
    "import ConfigParser\n",
    "from TwitterAPI import TwitterAPI\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tarfile\n",
    "from urllib import urlretrieve\n",
    "import pickle\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter(config_file):\n",
    "    \"\"\"Function to parse the Config file to get the Twitter OAuth Keys\n",
    "    \"\"\"\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "twitter = get_twitter('twitter.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      resource ... A resource string to request.\n",
    "      params ..... A parameter dictionary for the request.\n",
    "      max_tries .. The maximum number of tries to attempt.\n",
    "    Returns:\n",
    "      A TwitterResponse object, or None if failed.\n",
    "    \"\"\"\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print >> sys.stderr, 'Got error:', request.text, '\\nsleeping for 15 minutes.'\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_tweets(twitter, limit):\n",
    "    \"\"\"Data Collection function , this fucntion uses the Twitter API's 'statuses/user_timeline' field,\n",
    "    to collect the Public tweets for given handles\n",
    "    Params: \n",
    "    1.Twitter API\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    data=[]\n",
    "    n_loops=16;\n",
    "    for key,value in read_handles():\n",
    "        print key\n",
    "        for i_loop in range(1, n_loops):\n",
    "            tweets = robust_request(twitter,'statuses/user_timeline', {'screen_name':key,'count':200,'page':i_loop})\n",
    "            #pickle.dump(key+\".pkl\",tweets)\n",
    "            data.append(tweets)\n",
    "    return data\n",
    "\n",
    "twitter_data_collected = sample_tweets(twitter, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dump(filename,tweets):\n",
    "    \"\"\"Dumping the Tweets as .pkl for future usage\n",
    "    Prams:\n",
    "    Filename: The name of the pkl file in which tweets are to be saved\n",
    "    \"\"\"\n",
    "    for t in tweets:\n",
    "        print t['text']\n",
    "    pickle.dump(tweets, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inventory mechanism to store the data as .txt for easy manual labelling\n",
    "\"\"\"\n",
    "import io\n",
    "from collections import Counter\n",
    "globallist = io.open('list.txt',\"w\")\n",
    "for t in twitter_data_collected:\n",
    "    \n",
    "    c = []\n",
    "    for t1 in t:\n",
    "        if not(os.path.isdir(os.path.join(os.path.curdir+os.sep+'test'+os.sep+t1['user']['screen_name']))):\n",
    "            os.makedirs(os.path.join(os.path.curdir+os.sep+'test'+os.sep+t1['user']['screen_name'])) \n",
    "        file = io.open(os.path.join(os.path.curdir+os.sep+'test'+os.sep+t1['user']['screen_name']+os.sep+t1['id_str']+'.txt'),\"w\",encoding='utf8')\n",
    "        file.write(t1['text'])\n",
    "        globallist.write(os.path.curdir+os.sep+t1['user']['screen_name']+os.sep+t1['id_str']+'\\n')\n",
    "        file.close()\n",
    "globallist.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
